{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da0b8a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyarrow\n",
    "#!pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24623c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5529942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = Path().resolve().parent.parent\n",
    "data_dir = str(HOME)\n",
    "input_dir = data_dir + '/data/backup/'\n",
    "output_dir = data_dir + '/data/processed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df14c7da",
   "metadata": {},
   "source": [
    "### 1. Read all the files into a single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab55dcfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILEDATE \tMIN_DATE \tMAX_DATE \tDATASET_SHAPE\n",
      "1901 \t\t 2019-01-14 \t 2019-01-15 \t (18033, 107)\n",
      "1902 \t\t 2019-02-06 \t 2019-02-06 \t (17763, 107)\n",
      "1903 \t\t 2019-03-08 \t 2019-03-08 \t (17807, 107)\n",
      "1904 \t\t 2019-04-10 \t 2019-04-10 \t (17899, 107)\n",
      "1905 \t\t 2019-05-14 \t 2019-05-14 \t (18302, 107)\n",
      "1906 \t\t 2019-06-07 \t 2019-06-08 \t (18837, 107)\n",
      "1907 \t\t 2019-07-10 \t 2019-07-12 \t (19833, 107)\n",
      "1908 \t\t 2019-08-12 \t 2019-08-12 \t (20556, 107)\n",
      "1909 \t\t 2019-09-17 \t 2019-10-08 \t (20404, 107)\n",
      "1910 \t\t 2019-10-16 \t 2019-10-16 \t (20147, 107)\n",
      "1911 \t\t 2019-11-09 \t 2019-12-03 \t (20428, 107)\n",
      "1912 \t\t 2019-12-10 \t 2019-12-10 \t (20843, 107)\n",
      "2001 \t\t 2020-01-10 \t 2020-02-01 \t (20708, 107)\n",
      "2002 \t\t 2020-02-16 \t 2020-02-29 \t (20981, 107)\n",
      "2003 \t\t 2020-03-16 \t 94% \t (21117, 75)\n",
      "2004 \t\t 2020-04-16 \t 93% \t (20839, 75)\n",
      "2005 \t\t 2020-05-11 \t 92% \t (20859, 75)\n",
      "2006 \t\t 2020-06-13 \t 94% \t (20865, 75)\n",
      "2007 \t\t 2020-07-17 \t 96% \t (20518, 75)\n",
      "2008 \t\t 2020-08-24 \t 95% \t (20704, 75)\n",
      "2009 \t\t 2020-09-12 \t 94% \t (20338, 75)\n",
      "2010 \t\t 2020-10-12 \t 2020-10-25 \t (19896, 77)\n",
      "2011 \t\t 2020-11-06 \t 2020-11-06 \t (19896, 77)\n",
      "2012 \t\t 2020-12-16 \t 2020-12-30 \t (19641, 77)\n",
      "2101 \t\t 2021-01-12 \t 2021-01-14 \t (18569, 77)\n",
      "2102 \t\t 2021-02-09 \t 2021-02-11 \t (18471, 77)\n",
      "2103 \t\t 2021-03-05 \t 2021-03-12 \t (18286, 77)\n",
      "2104 \t\t 2021-04-12 \t 2021-04-19 \t (18226, 77)\n"
     ]
    }
   ],
   "source": [
    "all_files = glob.glob(input_dir + \"*.csv.gz\")\n",
    "li = []\n",
    "regex = re.compile(r'\\d+')\n",
    "\n",
    "print('FILEDATE \\tMIN_DATE \\tMAX_DATE \\tDATASET_SHAPE')\n",
    "\n",
    "for filename in all_files:\n",
    "    \n",
    "    # Read the raw data\n",
    "    df = pd.read_csv(filename, index_col=None)\n",
    "    \n",
    "    # Save filename date for future analysis   \n",
    "    file_date = re.search(regex, filename).group(0)\n",
    "    df['file_date'] = file_date\n",
    "    \n",
    "    # Print data info for each file\n",
    "    print('{} \\t\\t {} \\t {} \\t {}'.format(file_date, df.last_scraped.min(), df.last_scraped.max(), df.shape))\n",
    "    \n",
    "    # Append the data in a list\n",
    "    li.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c495ae0",
   "metadata": {},
   "source": [
    "The are some trash in the ```last_scraped``` column and it must be cleaned later. Also, we are going to save the file date info for future analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f74d2dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 73 valid columns in the dataset.\n"
     ]
    }
   ],
   "source": [
    "valid_columns = set(li[0].columns.values)\n",
    "\n",
    "for df in li:\n",
    "    current_cols = set(df.columns.values)\n",
    "    valid_columns.intersection_update(current_cols)\n",
    "    \n",
    "print('There are {} valid columns in the dataset.'.format(len(valid_columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbc9aa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(li)):\n",
    "    li[i] = li[i][valid_columns]\n",
    "\n",
    "# Aggregate all the data into a sigle dataset:\n",
    "df = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54547044",
   "metadata": {},
   "source": [
    "### 2. Removing columns pointed by Pol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7adc09f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current dataset has 550766 rows and 24 columns.\n"
     ]
    }
   ],
   "source": [
    "# Removed 'bathrooms_text' and 'number_of_reviews_l30d' from cols2drop\n",
    "\n",
    "cols2drop = ['listing_url', 'scrape_id', 'name', 'description', 'neighborhood_overview', \n",
    "             'picture_url', 'host_id', 'host_url', 'host_name', 'host_about', \n",
    "             'host_response_time', 'host_response_rate', 'host_acceptance_rate', \n",
    "             'host_is_superhost', 'host_thumbnail_url', 'host_picture_url', \n",
    "             'host_listings_count', 'host_total_listings_count', 'host_verifications',\n",
    "             'host_has_profile_pic', 'host_identity_verified', \n",
    "             'minimum_nights', 'maximum_nights', 'host_location',\n",
    "             'minimum_minimum_nights', 'maximum_minimum_nights', 'minimum_maximum_nights', \n",
    "             'maximum_maximum_nights', 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', \n",
    "             'host_since', 'neighbourhood', 'calendar_updated', 'review_scores_communication', \n",
    "             'review_scores_location', 'license', 'calculated_host_listings_count',\n",
    "             'calculated_host_listings_count_entire_homes', \n",
    "             'calculated_host_listings_count_private_rooms', \n",
    "             'calculated_host_listings_count_shared_rooms', 'host_neighbourhood', \n",
    "             'neighbourhood_cleansed', 'review_scores_accuracy', 'review_scores_cleanliness', \n",
    "             'review_scores_checkin', 'review_scores_value', 'number_of_reviews_ltm', \n",
    "             'first_review', 'last_review', 'instant_bookable']\n",
    "\n",
    "listings = df.drop(columns=cols2drop).copy()\n",
    "rows, cols = listings.shape\n",
    "print('The current dataset has {} rows and {} columns.'.format(rows, cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "859d2c52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final dataset has 550766 rows and 24 columns.\n"
     ]
    }
   ],
   "source": [
    "rows, cols = listings.shape\n",
    "print('The final dataset has {} rows and {} columns.'.format(rows, cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecd3892",
   "metadata": {},
   "source": [
    "### Save the final, aggregated dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b10207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "listings.to_csv(output_dir + '/listings_agg.csv.gz', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
